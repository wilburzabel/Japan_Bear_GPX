import streamlit as st
import pandas as pd
import requests
import json
import gpxpy
from shapely.geometry import Point, LineString
from streamlit_folium import st_folium
import folium
from folium.plugins import MarkerCluster, FastMarkerCluster
import datetime

# ==========================================
# 0. é¡µé¢åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(
    page_title="æ—¥æœ¬ç†Šå‡ºæ²¡å®‰å…¨åœ°å›¾ (ç§‹ç”°+å±±æ¢¨)", 
    layout="wide", 
    page_icon="ğŸ»"
)

# ==========================================
# 1. æ•°æ®æŠ½å–ä¸æ¸…æ´—å±‚ (ETL)
# ==========================================

# --- A. åŠ è½½ç§‹ç”°å¿æ•°æ® (æœ¬åœ° bears.json) ---
@st.cache_data
def load_akita_data(filepath="bears.json"):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            # å°è¯•è§£æ JSON
            try:
                raw_data = json.load(f)
            except json.JSONDecodeError:
                st.error("âŒ `bears.json` æ–‡ä»¶æ ¼å¼é”™è¯¯ã€‚è¯·ç¡®ä¿åœ¨ Charles ä¸­ä½¿ç”¨çš„æ˜¯ 'Save Response Body'ï¼Œè€Œä¸æ˜¯ä¿å­˜æ•´ä¸ª Responseã€‚")
                return pd.DataFrame()
        
        # æ£€æŸ¥æ•°æ®ç»“æ„ (é€‚é… kumadas.net çš„ç»“æ„)
        if 'result' in raw_data:
            df = pd.DataFrame(raw_data['result'])
        else:
            st.warning("âš ï¸ `bears.json` ä¸­æ‰¾ä¸åˆ° 'result' å­—æ®µï¼Œè¯·æ£€æŸ¥æ•°æ®æºã€‚")
            return pd.DataFrame()
            
        # å­—æ®µæ ‡å‡†åŒ– (ç›®æ ‡: latitude, longitude, sighting_datetime, sighting_condition)
        # å‡è®¾ kumadas.net è¿”å›çš„å·²ç»æ˜¯æ ‡å‡†å­—æ®µï¼Œå¦‚æœä¸æ˜¯ï¼Œéœ€è¦åœ¨è¿™é‡Œ rename
        # è¿™é‡Œåšä¸€ç‚¹å®¹é”™å¤„ç†
        if 'latitude' not in df.columns and 'lat' in df.columns:
            df = df.rename(columns={'lat': 'latitude', 'lon': 'longitude', 'body': 'sighting_condition', 'date': 'sighting_datetime'})
            
        # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
        required_cols = ['latitude', 'longitude']
        if not all(col in df.columns for col in required_cols):
            st.warning("âš ï¸ ç§‹ç”°æ•°æ®ç¼ºå¤±ç»çº¬åº¦å­—æ®µã€‚")
            return pd.DataFrame()

        # æ•°æ®ç±»å‹æ¸…æ´—
        df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
        df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
        df['sighting_datetime'] = pd.to_datetime(df['sighting_datetime'], errors='coerce')
        
        # è¡¥å……ç¼ºå¤±å€¼
        if 'sighting_condition' not in df.columns:
            df['sighting_condition'] = "æ— è¯¦ç»†æè¿°"
        else:
            df['sighting_condition'] = df['sighting_condition'].fillna("æ— è¯¦ç»†æè¿°")

        # æ·»åŠ æ¥æºæ ‡ç­¾
        df['source'] = 'ç§‹ç”°å¿ (æœ¬åœ°)'
        
        return df[['latitude', 'longitude', 'sighting_datetime', 'sighting_condition', 'source']].dropna(subset=['latitude', 'longitude'])
        
    except FileNotFoundError:
        st.error("âŒ æ‰¾ä¸åˆ° `bears.json` æ–‡ä»¶ã€‚è¯·å°† Charles æŠ“åˆ°çš„æ•°æ®ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ã€‚")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"âŒ ç§‹ç”°æ•°æ®åŠ è½½æœªçŸ¥é”™è¯¯: {e}")
        return pd.DataFrame()

# --- B. åŠ è½½å±±æ¢¨å¿æ•°æ® (è¿œç¨‹ CKAN API) ---
@st.cache_data
def load_yamanashi_data():
    url = "https://catalog.dataplatform-yamanashi.jp/api/action/datastore_search"
    params = {
        "resource_id": "b4eb262f-07e0-4417-b24f-6b15844b4ac1",
        "limit": 5000 
    }
    
    try:
        response = requests.get(url, params=params, timeout=15) # è®¾ç½®è¶…æ—¶é˜²æ­¢å¡æ­»
        data = response.json()
        
        if 'result' in data and 'records' in data['result']:
            df = pd.DataFrame(data['result']['records'])
            
            # 1. å­—æ®µåæ˜ å°„ (åŸºäºä½ æä¾›çš„æ ·æœ¬)
            rename_map = {
                'ç·¯åº¦': 'latitude',
                'çµŒåº¦': 'longitude',
                'å¹´æœˆæ—¥': 'sighting_datetime' # æ ·æœ¬æ˜¾ç¤ºæ˜¯è¿™ä¸ªå­—æ®µ
            }
            df = df.rename(columns=rename_map)
            
            # å¦‚æœæ˜ å°„åæ²¡æœ‰æ‰¾åˆ°å…³é”®åˆ—ï¼Œè¯´æ˜ API å­—æ®µåå˜äº†ï¼Œæ‰“å°å‡ºæ¥è°ƒè¯•
            if 'latitude' not in df.columns:
                # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„åˆ—å
                possible_lats = ['lat', 'Lat', 'LAT', 'çº¬åº¦']
                for col in possible_lats:
                    if col in df.columns:
                        df = df.rename(columns={col: 'latitude'})
                        break
            
            # 2. ç±»å‹è½¬æ¢
            df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
            df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
            df['sighting_datetime'] = pd.to_datetime(df['sighting_datetime'], errors='coerce')
            
            # 3. æ™ºèƒ½æ„å»ºæè¿°å­—æ®µ (æ‹¼æ¥å¤šä¸ªå­—æ®µ)
            def make_description(row):
                muni = str(row.get('ç›®æ’ƒå¸‚ç”ºæ‘', ''))
                place = str(row.get('å ´æ‰€', ''))
                time = str(row.get('æ™‚é–“', ''))
                age = str(row.get('æ¨å®šå¹´é½¢', ''))
                count = str(row.get('ç›®æ’ƒé ­æ•°', ''))
                
                desc = f"{muni} {place}".strip()
                details = []
                if time and time != 'nan': details.append(time)
                if age and age != 'nan': details.append(age)
                if count and count != 'nan': details.append(f"{count}é ­")
                
                if details:
                    desc += f" ({', '.join(details)})"
                
                return desc if desc else "APIæ•°æ®æ— æè¿°"

            df['sighting_condition'] = df.apply(make_description, axis=1)
            
            # 4. æ¥æºæ ‡ç­¾
            df['source'] = 'å±±æ¢¨å¿ (API)'
            
            return df[['latitude', 'longitude', 'sighting_datetime', 'sighting_condition', 'source']].dropna(subset=['latitude', 'longitude'])
            
        return pd.DataFrame()
        
    except Exception as e:
        st.warning(f"âš ï¸ å±±æ¢¨å¿ API è¿æ¥å¤±è´¥ (å¯èƒ½æ˜¯ç½‘ç»œåŸå› ): {e}")
        return pd.DataFrame()

# ==========================================
# 2. ä¸»é€»è¾‘æ§åˆ¶å™¨
# ==========================================

st.title("ğŸ» æ—¥æœ¬ç†Šå‡ºæ²¡å®‰å…¨åœ°å›¾")
st.markdown("èåˆ **ç§‹ç”°å¿ (æœ¬åœ°åº“)** ä¸ **å±±æ¢¨å¿ (å®æ—¶API)** æ•°æ®ï¼Œæä¾›å…¨æ–¹ä½çš„å¾’æ­¥å®‰å…¨æ£€æµ‹ã€‚")

# --- åŠ è½½æ•°æ® ---
with st.spinner('æ­£åœ¨èåˆå¤šæºæ•°æ®...'):
    df_akita = load_akita_data()
    df_yamanashi = load_yamanashi_data()
    
    # åˆå¹¶
    all_bears = pd.concat([df_akita, df_yamanashi], ignore_index=True)

# --- å…¨å±€æ£€æŸ¥ ---
if all_bears.empty:
    st.error("âŒ æ‰€æœ‰æ•°æ®æºå‡åŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥ï¼š1. bears.json æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ï¼›2. ç½‘ç»œæ˜¯å¦èƒ½è®¿é—®å±±æ¢¨å¿ APIã€‚")
    st.stop()
else:
    st.success(f"âœ… æˆåŠŸåŠ è½½ {len(all_bears)} æ¡è®°å½• (ç§‹ç”°: {len(df_akita)}, å±±æ¢¨: {len(df_yamanashi)})")

# ==========================================
# 3. ä¾§è¾¹æ ï¼šæ—¶é—´è¿‡æ»¤å™¨
# ==========================================
with st.sidebar:
    st.header("â³ ç­›é€‰è®¾ç½®")
    
    # è¿‡æ»¤æ‰æ— æ•ˆæ—¶é—´
    valid_dates = all_bears['sighting_datetime'].dropna()
    
    if not valid_dates.empty:
        min_date = valid_dates.min().date()
        max_date = valid_dates.max().date()
        
        # é»˜è®¤æ˜¾ç¤ºæœ€è¿‘ 1 å¹´
        default_start = max_date - datetime.timedelta(days=365)
        if default_start < min_date: default_start = min_date

        date_range = st.date_input(
            "é€‰æ‹©ç›®å‡»æ—¶é—´èŒƒå›´",
            value=(default_start, max_date),
            min_value=min_date,
            max_value=max_date
        )
        
        if len(date_range) == 2:
            start_d, end_d = date_range
            filtered_df = all_bears[
                (all_bears['sighting_datetime'].dt.date >= start_d) & 
                (all_bears['sighting_datetime'].dt.date <= end_d)
            ].copy()
        else:
            filtered_df = all_bears.copy()
    else:
        st.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¶é—´å­—æ®µï¼Œæ— æ³•ç­›é€‰ã€‚")
        filtered_df = all_bears.copy()

    st.divider()
    st.caption("Developed with Streamlit")

# ==========================================
# 4. åœ°å›¾å¯è§†åŒ–æ ¸å¿ƒ
# ==========================================

# é¡µé¢ä¸»è¦å¸ƒå±€
col1, col2 = st.columns([3, 1])

with col1:
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  GPX è·¯çº¿æ–‡ä»¶ (å¼€å¯ç²¾å‡†æ£€æµ‹)", type=['gpx'])

# ç¡®å®šåœ°å›¾é»˜è®¤ä¸­å¿ƒ (ä¼˜å…ˆæ˜¾ç¤ºç­›é€‰åçš„æ•°æ®ä¸­å¿ƒï¼Œå¦åˆ™æ˜¾ç¤ºæ—¥æœ¬ä¸­å¿ƒ)
if not filtered_df.empty:
    center_lat = filtered_df['latitude'].mean()
    center_lon = filtered_df['longitude'].mean()
else:
    center_lat, center_lon = 36.2048, 138.2529

m = folium.Map(location=[center_lat, center_lon], zoom_start=7, tiles="OpenStreetMap")

# --- åœºæ™¯ A: è·¯çº¿æ£€æµ‹æ¨¡å¼ (ç”¨æˆ·ä¸Šä¼ äº† GPX) ---
if uploaded_file is not None:
    try:
        gpx = gpxpy.parse(uploaded_file)
        points = []
        for track in gpx.tracks:
            for segment in track.segments:
                for point in segment.points:
                    points.append((point.latitude, point.longitude))
        
        if points:
            # 1. ç»˜åˆ¶è·¯çº¿
            folium.PolyLine(points, color="blue", weight=4, opacity=0.7, tooltip="å¾’æ­¥è·¯çº¿").add_to(m)
            
            # 2. ç©ºé—´è®¡ç®— (Buffer)
            route_line = LineString(points)
            buffer_dist = 0.005 # çº¦ 500m
            route_buffer = route_line.buffer(buffer_dist)
            min_lon, min_lat, max_lon, max_lat = route_buffer.bounds
            
            # 3. ç²—ç­› (æå¤§æå‡æ€§èƒ½)
            candidates = filtered_df[
                (filtered_df['latitude'] >= min_lat) & (filtered_df['latitude'] <= max_lat) &
                (filtered_df['longitude'] >= min_lon) & (filtered_df['longitude'] <= max_lon)
            ]
            
            # 4. ç²¾ç¡®æ£€æµ‹
            dangerous_bears = []
            for idx, row in candidates.iterrows():
                if route_buffer.contains(Point(row['latitude'], row['longitude'])):
                    dangerous_bears.append(row)
            
            # 5. æ¸²æŸ“å±é™©ç‚¹
            for bear in dangerous_bears:
                # é¢œè‰²åŒºåˆ†ï¼šç§‹ç”°(çº¢), å±±æ¢¨(æ©™)
                color = "red" if "ç§‹ç”°" in bear['source'] else "orange"
                
                date_str = bear['sighting_datetime'].strftime('%Y-%m-%d %H:%M') if pd.notnull(bear['sighting_datetime']) else "æœªçŸ¥æ—¶é—´"
                
                popup_html = f"""
                <div style="font-family:sans-serif; width:200px;">
                    <b>{bear['source']}</b><br>
                    <span style="color:red;">âš ï¸ {date_str}</span><br>
                    <hr style="margin:5px 0;">
                    {bear['sighting_condition']}
                </div>
                """
                folium.Marker(
                    [bear['latitude'], bear['longitude']],
                    popup=folium.Popup(popup_html, max_width=250),
                    icon=folium.Icon(color=color, icon="paw", prefix='fa')
                ).add_to(m)
                
            m.fit_bounds(route_line.bounds)
            
            # ç»“æœæç¤º
            if dangerous_bears:
                st.error(f"âš ï¸ è­¦å‘Šï¼šåœ¨è·¯çº¿ 500ç±³ èŒƒå›´å†…å‘ç° {len(dangerous_bears)} æ¡ç†Šå‡ºæ²¡è®°å½•ï¼")
                with st.expander("æŸ¥çœ‹è¯¦ç»†åˆ—è¡¨", expanded=True):
                    st.dataframe(pd.DataFrame(dangerous_bears)[['sighting_datetime', 'source', 'sighting_condition']])
            else:
                st.success("âœ… è¯¥æ—¶é—´æ®µå†…ï¼Œè·¯çº¿å‘¨è¾¹å®‰å…¨ï¼ˆæ— è®°å½•ï¼‰ã€‚")
        else:
            st.warning("GPX æ–‡ä»¶ä¸­æœªè§£æåˆ°è·¯å¾„ç‚¹ã€‚")
            
    except Exception as e:
        st.error(f"GPX è§£æå¤±è´¥: {e}")

# --- åœºæ™¯ B: å…¨æ™¯æ¢ç´¢æ¨¡å¼ (é»˜è®¤) ---
else:
    if not filtered_df.empty:
        # ä½¿ç”¨ MarkerCluster å¤„ç†å¤§é‡æ•°æ®
        marker_cluster = MarkerCluster(name="ç†Šå‡ºæ²¡èšåˆç‚¹").add_to(m)
        
        # é™åˆ¶æ˜¾ç¤ºæ•°é‡é˜²æ­¢æµè§ˆå™¨å´©æºƒ (å¦‚æœè¶…è¿‡ 5000 æ¡)
        limit = 5000
        if len(filtered_df) > limit:
            st.info(f"ğŸ’¡ æ•°æ®é‡è¾ƒå¤§ï¼Œåœ°å›¾ä»…æ˜¾ç¤ºæœ€è¿‘çš„ {limit} æ¡è®°å½•ã€‚è¯·ä½¿ç”¨ä¾§è¾¹æ ç­›é€‰ç¼©çŸ­æ—¶é—´èŒƒå›´ã€‚")
            display_data = filtered_df.sort_values('sighting_datetime', ascending=False).head(limit)
        else:
            display_data = filtered_df
            
        for idx, row in display_data.iterrows():
            color = "red" if "ç§‹ç”°" in row['source'] else "orange"
            date_str = row['sighting_datetime'].strftime('%Y-%m-%d') if pd.notnull(row['sighting_datetime']) else ""
            
            # ç®€åŒ–çš„ Popup
            popup_content = f"<b>{date_str}</b><br>{row['sighting_condition']}"
            
            folium.Marker(
                location=[row['latitude'], row['longitude']],
                popup=folium.Popup(popup_content, max_width=200),
                icon=folium.Icon(color=color, icon="info-sign"),
            ).add_to(marker_cluster)

# æ¸²æŸ“åœ°å›¾
st_folium(m, width="100%", height=600)
