import streamlit as st
import pandas as pd
import requests
import json
import gpxpy
from shapely.geometry import Point, LineString
from streamlit_folium import st_folium
import folium
from folium.plugins import FastMarkerCluster
import datetime

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(page_title="æ—¥æœ¬ç†Šå‡ºæ²¡ç»¼åˆçœ‹æ¿", layout="wide", page_icon="ğŸ»")

# ==========================================
# 1. æ•°æ®æŠ½å–ä¸æ¸…æ´—å±‚ (ETL)
# ==========================================

# --- A. åŠ è½½ç§‹ç”°å¿æ•°æ® (æœ¬åœ° JSON) ---
@st.cache_data
def load_akita_data(filepath="bears.json"):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # ç§‹ç”°æ•°æ®åœ¨ 'result' åˆ—è¡¨ä¸­
        if 'result' not in raw_data:
            return pd.DataFrame()
            
        df = pd.DataFrame(raw_data['result'])
        
        # æ ‡å‡†åŒ–å­—æ®µå
        # ç›®æ ‡æ ¼å¼: latitude, longitude, sighting_datetime, sighting_condition, source
        # ç§‹ç”°æºå­—æ®µå·²ç»æ˜¯ latitude, longitudeï¼Œæ— éœ€æ”¹å
        
        # æ¸…æ´—æ•°æ®
        df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
        df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
        df['sighting_datetime'] = pd.to_datetime(df['sighting_datetime'], errors='coerce')
        
        # è¡¥å……æ¥æºæ ‡ç­¾
        df['source'] = 'ç§‹ç”°å¿ (æœ¬åœ°åº“)'
        
        # ç¡®ä¿æœ‰æè¿°å­—æ®µ
        if 'sighting_condition' not in df.columns:
            df['sighting_condition'] = 'æ— è¯¦ç»†æè¿°'
            
        # é€‰å–æ ‡å‡†åˆ—
        return df[['latitude', 'longitude', 'sighting_datetime', 'sighting_condition', 'source']].dropna()
        
    except Exception as e:
        st.error(f"ç§‹ç”°æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return pd.DataFrame()

# --- B. åŠ è½½å±±æ¢¨å¿æ•°æ® (è¿œç¨‹ CKAN API) ---
@st.cache_data
def load_yamanashi_data():
    url = "https://catalog.dataplatform-yamanashi.jp/api/action/datastore_search"
    params = {
        "resource_id": "b4eb262f-07e0-4417-b24f-6b15844b4ac1",
        "limit": 5000 # è·å– 5000 æ¡
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if 'result' in data and 'records' in data['result']:
            df = pd.DataFrame(data['result']['records'])
            
            # --- å…³é”®ï¼šå­—æ®µåæ˜ å°„ ---
            # è¿™é‡Œæ˜¯æ ¹æ®å¸¸è§çš„æ—¥æœ¬å¼€æ”¾æ•°æ®å­—æ®µè¿›è¡Œçš„çŒœæµ‹
            # å¦‚æœä¸æ˜¾ç¤ºæ•°æ®ï¼Œè¯·å…ˆæŸ¥çœ‹é¡µé¢ä¸Šæ‰“å°çš„ "åŸå§‹å­—æ®µå"
            rename_map = {
                'ç·¯åº¦': 'latitude',
                'çº¬åº¦': 'latitude', # å®¹é”™
                'Lat': 'latitude',
                
                'çµŒåº¦': 'longitude',
                'ç»åº¦': 'longitude',
                'Lon': 'longitude',
                
                'ç™ºç”Ÿæ—¥æ™‚': 'sighting_datetime',
                'æœˆæ—¥': 'sighting_datetime', # æŸäº›è¡¨å¯èƒ½åªæœ‰æœˆæ—¥
                
                'å‡ºæ²¡çŠ¶æ³': 'sighting_condition',
                'çŠ¶æ³': 'sighting_condition',
                'æ‘˜è¦': 'sighting_condition'
            }
            
            df = df.rename(columns=rename_map)
            
            # æ¸…æ´—æ•°æ®
            df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
            df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
            
            # æ—¶é—´å¤„ç†å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼Œè¿™é‡Œå…ˆç®€å•å°è¯•
            df['sighting_datetime'] = pd.to_datetime(df['sighting_datetime'], errors='coerce')
            
            # è¡¥å……æ¥æºæ ‡ç­¾
            df['source'] = 'å±±æ¢¨å¿ (Live API)'
            
            # å¡«å……ç¼ºå¤±çš„æè¿°
            if 'sighting_condition' not in df.columns:
                 # å°è¯•åˆå¹¶åœ°å€ä½œä¸ºæè¿°
                 possible_desc = ['å ´æ‰€', 'å¸‚ç”ºæ‘å', 'ä½æ‰€', 'address']
                 for col in possible_desc:
                     if col in df.columns:
                         df['sighting_condition'] = df[col]
                         break
                 else:
                     df['sighting_condition'] = "APIæ•°æ®æ— æè¿°"

            # é€‰å–æ ‡å‡†åˆ— (å¦‚æœ API ç¼ºå°‘æŸäº›åˆ—ï¼Œè¿™é‡Œå¯èƒ½ä¼šæŠ¥é”™ï¼Œæ‰€ä»¥åŠ ä¸ªæ£€æµ‹)
            required_cols = ['latitude', 'longitude', 'sighting_datetime', 'sighting_condition', 'source']
            for col in required_cols:
                if col not in df.columns:
                    df[col] = None # è¡¥å…¨ç¼ºå¤±åˆ—
            
            return df[required_cols].dropna(subset=['latitude', 'longitude'])
            
        return pd.DataFrame()
        
    except Exception as e:
        # ä¸ºäº†ä¸å½±å“ä¸»ç¨‹åºè¿è¡Œï¼ŒAPI å¤±è´¥åªæ‰“å°è­¦å‘Š
        st.warning(f"å±±æ¢¨å¿ API è¿æ¥å¤±è´¥æˆ–è§£æé”™è¯¯: {e}")
        return pd.DataFrame()

# ==========================================
# 2. ä¸»ç¨‹åºé€»è¾‘
# ==========================================

st.title("ğŸ» æ—¥æœ¬ç†Šå‡ºæ²¡ç»¼åˆæ£€æµ‹çœ‹æ¿")
st.caption("æ•°æ®æºèåˆï¼šç§‹ç”°å¿ (JSONæ–‡ä»¶) + å±±æ¢¨å¿ (å®æ—¶API)")

# --- 1. å¹¶è¡ŒåŠ è½½æ•°æ® ---
with st.spinner('æ­£åœ¨èåˆå¤šæºæ•°æ®...'):
    df_akita = load_akita_data()
    df_yamanashi = load_yamanashi_data()
    
    # åˆå¹¶æ•°æ®è¡¨
    all_bears = pd.concat([df_akita, df_yamanashi], ignore_index=True)

# æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
if all_bears.empty:
    st.error("âŒ æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ bears.json æ–‡ä»¶ä½ç½®æˆ– API è¿æ¥ã€‚")
    st.stop()

# --- 2. ä¾§è¾¹æ ï¼šå…¨å±€æ—¶é—´ç­›é€‰ ---
with st.sidebar:
    st.header("â³ ç­›é€‰è®¾ç½®")
    
    # ç§»é™¤ç©ºæ—¶é—´ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
    valid_dates = all_bears['sighting_datetime'].dropna()
    if not valid_dates.empty:
        min_date = valid_dates.min().date()
        max_date = valid_dates.max().date()
        
        # é»˜è®¤çœ‹æœ€è¿‘ 2 å¹´
        default_start = max_date - datetime.timedelta(days=730)
        if default_start < min_date: default_start = min_date

        date_range = st.date_input(
            "é€‰æ‹©æ—¥æœŸèŒƒå›´",
            value=(default_start, max_date),
            min_value=min_date,
            max_value=max_date
        )
        
        if len(date_range) == 2:
            start_d, end_d = date_range
            # æ‰§è¡Œç­›é€‰
            filtered_df = all_bears[
                (all_bears['sighting_datetime'].dt.date >= start_d) & 
                (all_bears['sighting_datetime'].dt.date <= end_d)
            ].copy()
        else:
            filtered_df = all_bears.copy()
    else:
        filtered_df = all_bears.copy()
        st.warning("æ•°æ®ä¸­æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„æ—¶é—´å­—æ®µï¼Œæ˜¾ç¤ºå…¨éƒ¨æ•°æ®ã€‚")

    # æ•°æ®æºç»Ÿè®¡
    st.divider()
    st.write("ğŸ“Š æ•°æ®æºç»Ÿè®¡:")
    source_counts = filtered_df['source'].value_counts()
    st.write(source_counts)

# --- 3. åœ°å›¾ä¸åˆ†æé€»è¾‘ ---

uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  GPX è·¯çº¿è¿›è¡Œå®‰å…¨æ£€æµ‹", type=['gpx'])

# ç¡®å®šåœ°å›¾ä¸­å¿ƒ
if not filtered_df.empty:
    center_lat = filtered_df['latitude'].mean()
    center_lon = filtered_df['longitude'].mean()
else:
    center_lat, center_lon = 36.2, 138.2 # æ—¥æœ¬ä¸­å¿ƒå¤§æ¦‚ä½ç½®

m = folium.Map(location=[center_lat, center_lon], zoom_start=7)

# === åœºæ™¯ A: è·¯çº¿æ£€æµ‹æ¨¡å¼ ===
if uploaded_file is not None:
    gpx = gpxpy.parse(uploaded_file)
    points = []
    for track in gpx.tracks:
        for segment in track.segments:
            for point in segment.points:
                points.append((point.latitude, point.longitude))
    
    if points:
        folium.PolyLine(points, color="blue", weight=4, opacity=0.7).add_to(m)
        
        # ç©ºé—´è®¡ç®—
        route_line = LineString(points)
        route_buffer = route_line.buffer(0.005) # 500m
        min_lon, min_lat, max_lon, max_lat = route_buffer.bounds
        
        # ç²—ç­›
        candidates = filtered_df[
            (filtered_df['latitude'] >= min_lat) & (filtered_df['latitude'] <= max_lat) &
            (filtered_df['longitude'] >= min_lon) & (filtered_df['longitude'] <= max_lon)
        ]
        
        dangerous_bears = []
        for idx, row in candidates.iterrows():
            if route_buffer.contains(Point(row['latitude'], row['longitude'])):
                dangerous_bears.append(row)
        
        # æ¸²æŸ“å±é™©ç‚¹
        for bear in dangerous_bears:
            # æ ¹æ®æ¥æºè®¾ç½®ä¸åŒé¢œè‰²
            icon_color = "red" if "ç§‹ç”°" in bear['source'] else "orange"
            
            popup_html = f"""
            <b>æ¥æº:</b> {bear['source']}<br>
            <b>æ—¶é—´:</b> {bear['sighting_datetime']}<br>
            <b>è¯¦æƒ…:</b> {bear['sighting_condition']}
            """
            folium.Marker(
                [bear['latitude'], bear['longitude']],
                popup=folium.Popup(popup_html, max_width=250),
                icon=folium.Icon(color=icon_color, icon="paw", prefix='fa')
            ).add_to(m)
            
        m.fit_bounds(route_line.bounds)
        
        if dangerous_bears:
            st.error(f"âš ï¸ åœ¨è·¯çº¿å‘¨è¾¹å‘ç° {len(dangerous_bears)} æ¬¡ç›®å‡»è®°å½•ï¼")
            st.dataframe(pd.DataFrame(dangerous_bears)[['sighting_datetime', 'source', 'sighting_condition']])
        else:
            st.success("âœ… è·¯çº¿å‘¨è¾¹æš‚æ— è®°å½•ã€‚")

# === åœºæ™¯ B: å…¨æ™¯æ¢ç´¢æ¨¡å¼ ===
# --- B. åŠ è½½å±±æ¢¨å¿æ•°æ® (è¿œç¨‹ CKAN API - é€‚é…ç‰ˆ) ---
@st.cache_data
def load_yamanashi_data():
    # æ³¨æ„ï¼šURL å’Œ Resource ID ä¿æŒä¸å˜
    url = "https://catalog.dataplatform-yamanashi.jp/api/action/datastore_search"
    params = {
        "resource_id": "b4eb262f-07e0-4417-b24f-6b15844b4ac1",
        "limit": 5000 
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        
        if 'result' in data and 'records' in data['result']:
            df = pd.DataFrame(data['result']['records'])
            
            # 1. å­—æ®µåæ˜ å°„ (æ ¹æ®ä½ æä¾›çš„æ ·æœ¬ä¿®æ”¹)
            rename_map = {
                'ç·¯åº¦': 'latitude',
                'çµŒåº¦': 'longitude',
                'å¹´æœˆæ—¥': 'sighting_datetime' # ä½¿ç”¨è¿™ä¸ª ISO æ ¼å¼çš„æ—¥æœŸ
            }
            df = df.rename(columns=rename_map)
            
            # 2. æ•°æ®ç±»å‹è½¬æ¢
            df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')
            df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
            df['sighting_datetime'] = pd.to_datetime(df['sighting_datetime'], errors='coerce')
            
            # 3. å…³é”®ä¿®æ”¹ï¼šæ„å»ºâ€œç›®å‡»è¯¦æƒ…â€å­—æ®µ
            # å› ä¸ºåŸå§‹æ•°æ®æ²¡æœ‰å•ä¸€çš„â€œè¯¦æƒ…â€åˆ—ï¼Œæˆ‘ä»¬å°†å¤šä¸ªå­—æ®µæ‹¼æ¥èµ·æ¥ï¼Œåšæˆä¸€ä¸ªæ˜“è¯»çš„å­—ç¬¦ä¸²
            def make_description(row):
                # è·å–å„ä¸ªå­—æ®µï¼Œå¦‚æœä¸ºç©ºåˆ™æ˜¾ç¤ºç©ºå­—ç¬¦ä¸²
                muni = str(row.get('ç›®æ’ƒå¸‚ç”ºæ‘', ''))
                place = str(row.get('å ´æ‰€', ''))
                time = str(row.get('æ™‚é–“', ''))
                age = str(row.get('æ¨å®šå¹´é½¢', ''))
                count = str(row.get('ç›®æ’ƒé ­æ•°', ''))
                
                # æ‹¼æ¥æˆç±»ä¼¼: "æ—©å·ç”º åƒé ˆå’Œåœ°å†… (19:00, ã‚³ãƒ‰ãƒ¢, 1é ­)"
                desc = f"{muni} {place}"
                details = []
                if time and time != 'nan': details.append(time)
                if age and age != 'nan': details.append(age)
                if count and count != 'nan': details.append(f"{count}é ­")
                
                if details:
                    desc += f" ({', '.join(details)})"
                return desc

            # åº”ç”¨æ‹¼æ¥å‡½æ•°
            df['sighting_condition'] = df.apply(make_description, axis=1)
            
            # 4. è¡¥å……æ¥æºæ ‡ç­¾
            df['source'] = 'å±±æ¢¨å¿ (Live API)'
            
            # 5. é€‰å–æ ‡å‡†åˆ—
            required_cols = ['latitude', 'longitude', 'sighting_datetime', 'sighting_condition', 'source']
            return df[required_cols].dropna(subset=['latitude', 'longitude'])
            
        return pd.DataFrame()
        
    except Exception as e:
        st.warning(f"å±±æ¢¨å¿ API æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return pd.DataFrame()

# --- è°ƒè¯•åŒºï¼šå¦‚æœæœ‰å±±æ¢¨æ•°æ®ä½†æ²¡æ˜¾ç¤ºï¼ŒæŸ¥çœ‹è¿™é‡Œ ---
with st.expander("ğŸ›  å¼€å‘è€…å·¥å…·ï¼šæŸ¥çœ‹åŸå§‹æ•°æ®å­—æ®µ"):
    if not df_yamanashi.empty:
        st.write("å±±æ¢¨å¿æ•°æ®é¢„è§ˆ (å‰3è¡Œ):", df_yamanashi.head(3))
    else:
        st.write("å±±æ¢¨å¿æ•°æ®ä¸ºç©º (è¯·æ£€æŸ¥ API æˆ– å­—æ®µæ˜ å°„)")
